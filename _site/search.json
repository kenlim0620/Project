[
  {
    "objectID": "user-guide.html",
    "href": "user-guide.html",
    "title": "User Guide",
    "section": "",
    "text": "This is SU HUI part"
  },
  {
    "objectID": "storyboard.html",
    "href": "storyboard.html",
    "title": "Project Storyboard",
    "section": "",
    "text": "Data without a narrative is just noise. Our storyboard maps out the complete journey of a FinTech customer‚Äîfrom their first deposit to their potential departure‚Äîand demonstrates how our Visual Analytics dashboard empowers stakeholders to intervene at the right moments.\n\n\n\nThe FinRetain project translates raw, high-dimensional transactional data into a visual story of customer behavior, liquidity flow, and churn risk. Instead of looking at isolated metrics, this storyboard walks through the holistic lifecycle of our user base, providing actionable insights at every stage.\n\n\n\n\nIn the highly competitive FinTech sector, customer acquisition is expensive, but customer retention is profitable. Currently, platform managers face a critical blind spot: they know when a user leaves, but they cannot visualize the behavioral degradation leading up to that moment.\nOur motivation is to replace reactive reporting with proactive, predictive visual analytics. We want to answer three questions: 1. Who are our most valuable users? 2. Where is the money flowing? 3. When and Why will a user leave?\n\n\n\n\nTo tell this story, we designed a pipeline that moves from the past (descriptive) to the future (predictive):\n\nPhase 1: Foundation (Data Prep & UI Design): Cleaning 10,000+ records and designing a low-cognitive-load interface.\nPhase 2: Discovery (EDA & CDA): Establishing demographic baselines and testing behavioral hypotheses.\nPhase 3: Segmentation (Cluster Analysis): Using K-Means and RFM to group users autonomously.\nPhase 4: Ecosystem Mapping: Deploying Sankey diagrams to trace macro cash flows.\nPhase 5: Forecasting (Predictive Modelling): Simulating future churn risk using dynamic regression.\n\n\n\n\n\n\n\n\nThe Visual: A dynamic grid of Box Plots and Bar Charts.\nThe Story: A new user joins the app. Through our EDA tabs, we immediately categorize their demographic profile (e.g., Age, Education, Income). We notice that high-income users tend to be older, setting the stage for how we market premium investment products to them.\n\n\n\n\n\nThe Visual: An interactive Scatter Plot with a Linear Trendline.\nThe Story: As the user interacts with the app, we track their transactions versus their satisfaction. The plot reveals a narrative twist: users who transact frequently but have high support ticket counts show plunging satisfaction. The UI highlights this friction point for the product team.\n\n\n\n\n\nThe Visual: A dense, color-coded RFM Treemap and K-Means Scatter Plot.\nThe Story: Months pass. The user‚Äôs behavior matures. Our K-Means algorithm automatically assigns them to the ‚ÄúHigh-Value/High-Frequency‚Äù cluster. In the Treemap, they appear as a large, dark-blue square. They are now a ‚ÄúChampion.‚Äù\n\n\n\n\n\nThe Visual: A Kaplan-Meier Survival Curve showing a steep drop at Month 3.\nThe Story: The narrative reaches its climax. The survival curve shows that even Champions are at severe risk of churning around the 90-day mark. This visual acts as a ticking clock, urging the marketing team to deploy retention incentives immediately at Month 2.\n\n\n\n\n\nThe Visual: A massive, flowing Sankey Diagram.\nThe Story: Zooming out from the individual user, we look at the ecosystem. If users churn, where does the money go? The Sankey traces capital from ‚ÄúDirect Deposits‚Äù directly out to ‚ÄúExternal Withdrawals.‚Äù It visualizes the exact dollar amount the company bleeds when retention fails.\n\n\n\n\n\n\nTo change the ending of this story, we step into the Risk Simulator. Instead of accepting the churn predicted in Scene 4, a stakeholder adjusts the UI sliders‚Äîsimulating a scenario where the user receives faster customer support and automated savings features.\nThe Result: The model recalculates on the fly. The Churn Probability gauge drops from Red (85%) to Green (12%), and the predicted Customer Lifetime Value (CLV) increases. We have visually forecasted a saved customer.\n\n\n\n\nThe FinRetain dashboard proves that Visual Analytics is more than just plotting charts; it is about building a decision-making engine. By combining exploratory data analysis, algorithmic clustering, and predictive forecasting into a single, cohesive storyboard, we empower FinTech platforms to protect their most valuable asset: their users."
  },
  {
    "objectID": "storyboard.html#introduction",
    "href": "storyboard.html#introduction",
    "title": "Project Storyboard",
    "section": "",
    "text": "The FinRetain project translates raw, high-dimensional transactional data into a visual story of customer behavior, liquidity flow, and churn risk. Instead of looking at isolated metrics, this storyboard walks through the holistic lifecycle of our user base, providing actionable insights at every stage."
  },
  {
    "objectID": "storyboard.html#motivation",
    "href": "storyboard.html#motivation",
    "title": "Project Storyboard",
    "section": "",
    "text": "In the highly competitive FinTech sector, customer acquisition is expensive, but customer retention is profitable. Currently, platform managers face a critical blind spot: they know when a user leaves, but they cannot visualize the behavioral degradation leading up to that moment.\nOur motivation is to replace reactive reporting with proactive, predictive visual analytics. We want to answer three questions: 1. Who are our most valuable users? 2. Where is the money flowing? 3. When and Why will a user leave?"
  },
  {
    "objectID": "storyboard.html#methodology",
    "href": "storyboard.html#methodology",
    "title": "Project Storyboard",
    "section": "",
    "text": "To tell this story, we designed a pipeline that moves from the past (descriptive) to the future (predictive):\n\nPhase 1: Foundation (Data Prep & UI Design): Cleaning 10,000+ records and designing a low-cognitive-load interface.\nPhase 2: Discovery (EDA & CDA): Establishing demographic baselines and testing behavioral hypotheses.\nPhase 3: Segmentation (Cluster Analysis): Using K-Means and RFM to group users autonomously.\nPhase 4: Ecosystem Mapping: Deploying Sankey diagrams to trace macro cash flows.\nPhase 5: Forecasting (Predictive Modelling): Simulating future churn risk using dynamic regression."
  },
  {
    "objectID": "storyboard.html#the-storyboard-a-users-journey",
    "href": "storyboard.html#the-storyboard-a-users-journey",
    "title": "Project Storyboard",
    "section": "",
    "text": "The Visual: A dynamic grid of Box Plots and Bar Charts.\nThe Story: A new user joins the app. Through our EDA tabs, we immediately categorize their demographic profile (e.g., Age, Education, Income). We notice that high-income users tend to be older, setting the stage for how we market premium investment products to them.\n\n\n\n\n\nThe Visual: An interactive Scatter Plot with a Linear Trendline.\nThe Story: As the user interacts with the app, we track their transactions versus their satisfaction. The plot reveals a narrative twist: users who transact frequently but have high support ticket counts show plunging satisfaction. The UI highlights this friction point for the product team.\n\n\n\n\n\nThe Visual: A dense, color-coded RFM Treemap and K-Means Scatter Plot.\nThe Story: Months pass. The user‚Äôs behavior matures. Our K-Means algorithm automatically assigns them to the ‚ÄúHigh-Value/High-Frequency‚Äù cluster. In the Treemap, they appear as a large, dark-blue square. They are now a ‚ÄúChampion.‚Äù\n\n\n\n\n\nThe Visual: A Kaplan-Meier Survival Curve showing a steep drop at Month 3.\nThe Story: The narrative reaches its climax. The survival curve shows that even Champions are at severe risk of churning around the 90-day mark. This visual acts as a ticking clock, urging the marketing team to deploy retention incentives immediately at Month 2.\n\n\n\n\n\nThe Visual: A massive, flowing Sankey Diagram.\nThe Story: Zooming out from the individual user, we look at the ecosystem. If users churn, where does the money go? The Sankey traces capital from ‚ÄúDirect Deposits‚Äù directly out to ‚ÄúExternal Withdrawals.‚Äù It visualizes the exact dollar amount the company bleeds when retention fails."
  },
  {
    "objectID": "storyboard.html#forecasting-simulation",
    "href": "storyboard.html#forecasting-simulation",
    "title": "Project Storyboard",
    "section": "",
    "text": "To change the ending of this story, we step into the Risk Simulator. Instead of accepting the churn predicted in Scene 4, a stakeholder adjusts the UI sliders‚Äîsimulating a scenario where the user receives faster customer support and automated savings features.\nThe Result: The model recalculates on the fly. The Churn Probability gauge drops from Red (85%) to Green (12%), and the predicted Customer Lifetime Value (CLV) increases. We have visually forecasted a saved customer."
  },
  {
    "objectID": "storyboard.html#conclusion",
    "href": "storyboard.html#conclusion",
    "title": "Project Storyboard",
    "section": "",
    "text": "The FinRetain dashboard proves that Visual Analytics is more than just plotting charts; it is about building a decision-making engine. By combining exploratory data analysis, algorithmic clustering, and predictive forecasting into a single, cohesive storyboard, we empower FinTech platforms to protect their most valuable asset: their users."
  },
  {
    "objectID": "poster.html",
    "href": "poster.html",
    "title": "Project Deliverables - Poster",
    "section": "",
    "text": "This is SU HUI part"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Democratising FinTech Data through Visual Analytics",
    "section": "",
    "text": "Our project, FinRetain, leverages Exploratory Data Analysis, K-Means Clustering, and Predictive Modelling to transform static transactional data into actionable business intelligence. Built for ISSS608 Visual Analytics.\nView Project Proposal\nLaunch Interactive Dashboard"
  },
  {
    "objectID": "index.html#eda-cda",
    "href": "index.html#eda-cda",
    "title": "Democratising FinTech Data through Visual Analytics",
    "section": "üìä EDA & CDA",
    "text": "üìä EDA & CDA\nRigorous data preparation to establish demographic baselines and confirm behavioral hypotheses using interactive visualization."
  },
  {
    "objectID": "index.html#clustering-modelling",
    "href": "index.html#clustering-modelling",
    "title": "Democratising FinTech Data through Visual Analytics",
    "section": "üß† Clustering & Modelling",
    "text": "üß† Clustering & Modelling\nDeploying K-Means algorithms to segment users, paired with Kaplan-Meier Survival Models and Sankey Networks to trace capital flow."
  },
  {
    "objectID": "index.html#predictive-simulation",
    "href": "index.html#predictive-simulation",
    "title": "Democratising FinTech Data through Visual Analytics",
    "section": "üîÆ Predictive Simulation",
    "text": "üîÆ Predictive Simulation\nAn interactive Risk Simulator powered by regression models, allowing stakeholders to dynamically forecast Churn Probability and CLV."
  },
  {
    "objectID": "data_prep.html",
    "href": "data_prep.html",
    "title": "Data Preparation & Feature Engineering",
    "section": "",
    "text": "To ensure the FinRetain Shiny application runs at lightning speed on the web, we intentionally decoupled the heavy data processing from the live application. Our data preparation pipeline cleans raw demographic and transactional data, performs advanced feature engineering, and exports a highly optimized .rds file. Below is the step-by-step methodology used to prepare the dataset.\n\n\n\nFirst, we began by loading the raw simulated customer dataset. Categorical variables with intrinsic hierarchy needed to be properly factored so that visual plots (like bar charts) would display in a logical order, rather than alphabetically.\n\n\nCode\nlibrary(dplyr)\nlibrary(lubridate)\nlibrary(tidyr)\nlibrary(readr)\nlibrary(readxl)\n\n# 1. Load raw data (Using read.csv for best compatibility)\nraw_customers &lt;- read.csv(\"customer_data.csv\")\nraw_transactions &lt;- read_xlsx(\"transactions_data.xlsx\")\n\n# 2. Inspect initial dimensions\ndim(raw_customers)\n\n\n[1] 48723    54\n\n\n\n\n\nMany of our visual models (like the EDA Boxplots and K-Means clusters) require specific data types. We need to convert character dates into actual Date objects and ensure categorical variables are treated as factors.\n\n\nCode\nclean_customers &lt;- raw_customers %&gt;%\n  # Convert date columns using lubridate\n  mutate(\n    first_tx = ymd(first_tx),\n    last_tx = ymd(last_tx),\n    last_survey_date = ymd(last_survey_date)\n  ) %&gt;%\n  # Convert categorical strings to factors for plotting and clustering\n  mutate(\n    education_level = factor(education_level, levels = c(\"High School\", \"Bachelor\", \"Master\", \"PhD\")),\n    customer_segment = as.factor(customer_segment),\n    gender = as.factor(gender),\n    income_bracket = factor(income_bracket, levels = c(\"Low\", \"Medium\", \"High\", \"Very High\"))\n  ) %&gt;%\n  # Handle Missing Values for critical numerical columns\n  mutate(\n    support_tickets_count = replace_na(support_tickets_count, 0),\n    tx_count = replace_na(tx_count, 0),\n    total_tx_volume = replace_na(total_tx_volume, 0)\n  )\n\n\n\n\n\nTo power our Advanced Analytics modules (Survival Analysis and RFM Treemap), we needed to dynamically generate new variables from the raw data.To support the advanced modules in our analysis.qmd file, we must mathematically derive several new variables.\n\n\nThe Kaplan-Meier Survival Curve requires a binary event indicator (1 = Churned, 0 = Active). We derive this from the customer_segment or churn_probability fields. It also requires a definitive ‚Äúevent‚Äù (1 for occurred, 0 for censored). Since our raw data provided a continuous churn_probability, we established a strict threshold: customers with a probability &gt; 60% were flagged as a definitive churn event.\n\n\nCode\nclean_customers &lt;- clean_customers %&gt;%\n  mutate(\n    # Create binary flag for the Survival Analysis Model\n    churn_event = ifelse(customer_segment == \"inactive\" | churn_probability &gt; 0.8, 1, 0),\n    # Ensure customer_tenure is numeric (measured in months)\n    customer_tenure = as.numeric(customer_tenure)\n  )\n\n\n\n\n\nOur interactive Treemap visualizes RFM segments. We must calculate the R, F, and M scores (1-5 scale) using quantiles, and then group customers into business logic categories. We calculated RFM scores by dividing the customer base into tertiles (using ntile()) across three key dimensions:\n\nRecency (R): Based on customer_tenure (proxy for how long they‚Äôve stayed active).\nFrequency (F): Based on tx_count (Total Transactions).\nMonetary (M): Based on total_tx_volume (Total volume moved).\n\n\n\n\nUsing the generated 1-5 scores, we applied conditional logic to segment the user base into actionable business categories:\n\n\nCode\n# Define the \"current date\" for Recency calculation as the max date in the dataset\nreference_date &lt;- max(clean_customers$last_tx, na.rm = TRUE)\n\nclean_customers &lt;- clean_customers %&gt;%\n  mutate(\n    # 1. Calculate Recency (Days since last transaction)\n    recency_days = as.numeric(difftime(reference_date, last_tx, units = \"days\"))\n  ) %&gt;%\n  mutate(\n    # 2. Assign Quantile Scores (1 to 5)\n    R_Score = ntile(-recency_days, 5), # Lower days = Higher Score (5)\n    F_Score = ntile(tx_count, 5),      # Higher count = Higher Score (5)\n    M_Score = ntile(total_tx_volume, 5) # Higher volume = Higher Score (5)\n  ) %&gt;%\n  mutate(\n    # 3. Calculate Average RFM Score\n    RFM_Avg = (R_Score + F_Score + M_Score) / 3,\n    \n    # 4. Define Segments using Business Logic\n    RFM_Segment = case_when(\n      R_Score &gt;= 4 & F_Score &gt;= 4 & M_Score &gt;= 4 ~ \"Champions\",\n      R_Score &gt;= 3 & F_Score &gt;= 3 ~ \"Loyal Customers\",\n      R_Score &lt;= 2 & F_Score &gt;= 3 ~ \"At Risk\",\n      R_Score &lt;= 2 & F_Score &lt;= 2 ~ \"Lost / Inactive\",\n      TRUE ~ \"Average Users\"\n    )\n  )\n\n\n\n\n\n\nOnce the dataset was cleaned and enriched with our engineered features, we saved it as an R Data Serialization (.rds) file. This format preserves all R-specific attributes (like our factored income levels) and loads significantly faster in the Shiny Server environment than a standard CSV.\n\n\nCode\nlibrary(dplyr)\nlibrary(lubridate)\nlibrary(tidyr)\nlibrary(readr)\n\n# 1. Load raw data\nraw_customers &lt;- read.csv(\"customer_data.csv\")\nraw_transactions &lt;- read_xlsx(\"transactions_data.xlsx\")\n\n# 2. Get the reference date for RFM calculation BEFORE the pipeline\nreference_date &lt;- max(ymd(raw_customers$last_tx), na.rm = TRUE)\n\n# 3. The Unified ETL Pipeline\nclean_customers &lt;- raw_customers %&gt;%\n  \n  # --- STEP A: Clean Dates & Types ---\n  mutate(\n    first_tx = ymd(first_tx),\n    last_tx = ymd(last_tx),\n    last_survey_date = ymd(last_survey_date),\n    education_level = factor(education_level, levels = c(\"High School\", \"Bachelor\", \"Master\", \"PhD\")),\n    customer_segment = as.factor(customer_segment),\n    gender = as.factor(gender),\n    income_bracket = factor(income_bracket, levels = c(\"Low\", \"Medium\", \"High\", \"Very High\")),\n    customer_tenure = as.numeric(customer_tenure)\n  ) %&gt;%\n  \n  # --- STEP B: Handle Missing Values ---\n  mutate(\n    support_tickets_count = replace_na(support_tickets_count, 0),\n    tx_count = replace_na(tx_count, 0),\n    total_tx_volume = replace_na(total_tx_volume, 0)\n  ) %&gt;%\n  \n  # --- STEP C: Survival Analysis Feature (Fixes your error!) ---\n  mutate(\n    churn_event = ifelse(customer_segment == \"inactive\" | churn_probability &gt; 0.8, 1, 0)\n  ) %&gt;%\n  \n  # --- STEP D: RFM Segmentation ---\n  mutate(\n    recency_days = as.numeric(difftime(reference_date, last_tx, units = \"days\")),\n    R_Score = ntile(-recency_days, 5),\n    F_Score = ntile(tx_count, 5),\n    M_Score = ntile(total_tx_volume, 5)\n  ) %&gt;%\n  mutate(\n    RFM_Segment = case_when(\n      R_Score &gt;= 4 & F_Score &gt;= 4 & M_Score &gt;= 4 ~ \"Champions\",\n      R_Score &gt;= 3 & F_Score &gt;= 3 ~ \"Loyal Customers\",\n      R_Score &lt;= 2 & F_Score &gt;= 3 ~ \"At Risk\",\n      R_Score &lt;= 2 & F_Score &lt;= 2 ~ \"Lost / Inactive\",\n      TRUE ~ \"Average Users\"\n    )\n  )\n\n# 4. Save the final processed data\nsaveRDS(clean_customers, file = \"app_data.rds\")\n\n# 5. Verify it worked perfectly!\nsummary(clean_customers %&gt;% select(churn_event, RFM_Segment, tx_count, total_tx_volume))\n\n\n  churn_event     RFM_Segment           tx_count        total_tx_volume    \n Min.   :0.0000   Length:48723       Min.   :   10.00   Min.   :3.182e+06  \n 1st Qu.:0.0000   Class :character   1st Qu.:   12.00   1st Qu.:2.016e+07  \n Median :0.0000   Mode  :character   Median :   18.00   Median :4.808e+07  \n Mean   :0.2033                      Mean   :   64.84   Mean   :2.575e+08  \n 3rd Qu.:0.0000                      3rd Qu.:   35.00   3rd Qu.:1.204e+08  \n Max.   :1.0000                      Max.   :77465.00   Max.   :1.073e+12  \n\n\n\n\nThe dataset is now completely formatted. It includes the RFM_Segment for our Treemap, churn_event for the Kaplan-Meier survival curves, and strictly typed numeric columns for the K-Means clustering and predictive regression models."
  },
  {
    "objectID": "data_prep.html#step-1.-data-extraction",
    "href": "data_prep.html#step-1.-data-extraction",
    "title": "Data Preparation & Feature Engineering",
    "section": "",
    "text": "First, we began by loading the raw simulated customer dataset. Categorical variables with intrinsic hierarchy needed to be properly factored so that visual plots (like bar charts) would display in a logical order, rather than alphabetically.\n\n\nCode\nlibrary(dplyr)\nlibrary(lubridate)\nlibrary(tidyr)\nlibrary(readr)\nlibrary(readxl)\n\n# 1. Load raw data (Using read.csv for best compatibility)\nraw_customers &lt;- read.csv(\"customer_data.csv\")\nraw_transactions &lt;- read_xlsx(\"transactions_data.xlsx\")\n\n# 2. Inspect initial dimensions\ndim(raw_customers)\n\n\n[1] 48723    54"
  },
  {
    "objectID": "data_prep.html#step-2.-data-cleaning-type-conversion-formatting",
    "href": "data_prep.html#step-2.-data-cleaning-type-conversion-formatting",
    "title": "Data Preparation & Feature Engineering",
    "section": "",
    "text": "Many of our visual models (like the EDA Boxplots and K-Means clusters) require specific data types. We need to convert character dates into actual Date objects and ensure categorical variables are treated as factors.\n\n\nCode\nclean_customers &lt;- raw_customers %&gt;%\n  # Convert date columns using lubridate\n  mutate(\n    first_tx = ymd(first_tx),\n    last_tx = ymd(last_tx),\n    last_survey_date = ymd(last_survey_date)\n  ) %&gt;%\n  # Convert categorical strings to factors for plotting and clustering\n  mutate(\n    education_level = factor(education_level, levels = c(\"High School\", \"Bachelor\", \"Master\", \"PhD\")),\n    customer_segment = as.factor(customer_segment),\n    gender = as.factor(gender),\n    income_bracket = factor(income_bracket, levels = c(\"Low\", \"Medium\", \"High\", \"Very High\"))\n  ) %&gt;%\n  # Handle Missing Values for critical numerical columns\n  mutate(\n    support_tickets_count = replace_na(support_tickets_count, 0),\n    tx_count = replace_na(tx_count, 0),\n    total_tx_volume = replace_na(total_tx_volume, 0)\n  )"
  },
  {
    "objectID": "data_prep.html#step-3-feature-engineering-advanced-metrics",
    "href": "data_prep.html#step-3-feature-engineering-advanced-metrics",
    "title": "Data Preparation & Feature Engineering",
    "section": "",
    "text": "To power our Advanced Analytics modules (Survival Analysis and RFM Treemap), we needed to dynamically generate new variables from the raw data.To support the advanced modules in our analysis.qmd file, we must mathematically derive several new variables.\n\n\nThe Kaplan-Meier Survival Curve requires a binary event indicator (1 = Churned, 0 = Active). We derive this from the customer_segment or churn_probability fields. It also requires a definitive ‚Äúevent‚Äù (1 for occurred, 0 for censored). Since our raw data provided a continuous churn_probability, we established a strict threshold: customers with a probability &gt; 60% were flagged as a definitive churn event.\n\n\nCode\nclean_customers &lt;- clean_customers %&gt;%\n  mutate(\n    # Create binary flag for the Survival Analysis Model\n    churn_event = ifelse(customer_segment == \"inactive\" | churn_probability &gt; 0.8, 1, 0),\n    # Ensure customer_tenure is numeric (measured in months)\n    customer_tenure = as.numeric(customer_tenure)\n  )\n\n\n\n\n\nOur interactive Treemap visualizes RFM segments. We must calculate the R, F, and M scores (1-5 scale) using quantiles, and then group customers into business logic categories. We calculated RFM scores by dividing the customer base into tertiles (using ntile()) across three key dimensions:\n\nRecency (R): Based on customer_tenure (proxy for how long they‚Äôve stayed active).\nFrequency (F): Based on tx_count (Total Transactions).\nMonetary (M): Based on total_tx_volume (Total volume moved).\n\n\n\n\nUsing the generated 1-5 scores, we applied conditional logic to segment the user base into actionable business categories:\n\n\nCode\n# Define the \"current date\" for Recency calculation as the max date in the dataset\nreference_date &lt;- max(clean_customers$last_tx, na.rm = TRUE)\n\nclean_customers &lt;- clean_customers %&gt;%\n  mutate(\n    # 1. Calculate Recency (Days since last transaction)\n    recency_days = as.numeric(difftime(reference_date, last_tx, units = \"days\"))\n  ) %&gt;%\n  mutate(\n    # 2. Assign Quantile Scores (1 to 5)\n    R_Score = ntile(-recency_days, 5), # Lower days = Higher Score (5)\n    F_Score = ntile(tx_count, 5),      # Higher count = Higher Score (5)\n    M_Score = ntile(total_tx_volume, 5) # Higher volume = Higher Score (5)\n  ) %&gt;%\n  mutate(\n    # 3. Calculate Average RFM Score\n    RFM_Avg = (R_Score + F_Score + M_Score) / 3,\n    \n    # 4. Define Segments using Business Logic\n    RFM_Segment = case_when(\n      R_Score &gt;= 4 & F_Score &gt;= 4 & M_Score &gt;= 4 ~ \"Champions\",\n      R_Score &gt;= 3 & F_Score &gt;= 3 ~ \"Loyal Customers\",\n      R_Score &lt;= 2 & F_Score &gt;= 3 ~ \"At Risk\",\n      R_Score &lt;= 2 & F_Score &lt;= 2 ~ \"Lost / Inactive\",\n      TRUE ~ \"Average Users\"\n    )\n  )"
  },
  {
    "objectID": "data_prep.html#step-4-final-verification-and-exporting-for-production",
    "href": "data_prep.html#step-4-final-verification-and-exporting-for-production",
    "title": "Data Preparation & Feature Engineering",
    "section": "",
    "text": "Once the dataset was cleaned and enriched with our engineered features, we saved it as an R Data Serialization (.rds) file. This format preserves all R-specific attributes (like our factored income levels) and loads significantly faster in the Shiny Server environment than a standard CSV.\n\n\nCode\nlibrary(dplyr)\nlibrary(lubridate)\nlibrary(tidyr)\nlibrary(readr)\n\n# 1. Load raw data\nraw_customers &lt;- read.csv(\"customer_data.csv\")\nraw_transactions &lt;- read_xlsx(\"transactions_data.xlsx\")\n\n# 2. Get the reference date for RFM calculation BEFORE the pipeline\nreference_date &lt;- max(ymd(raw_customers$last_tx), na.rm = TRUE)\n\n# 3. The Unified ETL Pipeline\nclean_customers &lt;- raw_customers %&gt;%\n  \n  # --- STEP A: Clean Dates & Types ---\n  mutate(\n    first_tx = ymd(first_tx),\n    last_tx = ymd(last_tx),\n    last_survey_date = ymd(last_survey_date),\n    education_level = factor(education_level, levels = c(\"High School\", \"Bachelor\", \"Master\", \"PhD\")),\n    customer_segment = as.factor(customer_segment),\n    gender = as.factor(gender),\n    income_bracket = factor(income_bracket, levels = c(\"Low\", \"Medium\", \"High\", \"Very High\")),\n    customer_tenure = as.numeric(customer_tenure)\n  ) %&gt;%\n  \n  # --- STEP B: Handle Missing Values ---\n  mutate(\n    support_tickets_count = replace_na(support_tickets_count, 0),\n    tx_count = replace_na(tx_count, 0),\n    total_tx_volume = replace_na(total_tx_volume, 0)\n  ) %&gt;%\n  \n  # --- STEP C: Survival Analysis Feature (Fixes your error!) ---\n  mutate(\n    churn_event = ifelse(customer_segment == \"inactive\" | churn_probability &gt; 0.8, 1, 0)\n  ) %&gt;%\n  \n  # --- STEP D: RFM Segmentation ---\n  mutate(\n    recency_days = as.numeric(difftime(reference_date, last_tx, units = \"days\")),\n    R_Score = ntile(-recency_days, 5),\n    F_Score = ntile(tx_count, 5),\n    M_Score = ntile(total_tx_volume, 5)\n  ) %&gt;%\n  mutate(\n    RFM_Segment = case_when(\n      R_Score &gt;= 4 & F_Score &gt;= 4 & M_Score &gt;= 4 ~ \"Champions\",\n      R_Score &gt;= 3 & F_Score &gt;= 3 ~ \"Loyal Customers\",\n      R_Score &lt;= 2 & F_Score &gt;= 3 ~ \"At Risk\",\n      R_Score &lt;= 2 & F_Score &lt;= 2 ~ \"Lost / Inactive\",\n      TRUE ~ \"Average Users\"\n    )\n  )\n\n# 4. Save the final processed data\nsaveRDS(clean_customers, file = \"app_data.rds\")\n\n# 5. Verify it worked perfectly!\nsummary(clean_customers %&gt;% select(churn_event, RFM_Segment, tx_count, total_tx_volume))\n\n\n  churn_event     RFM_Segment           tx_count        total_tx_volume    \n Min.   :0.0000   Length:48723       Min.   :   10.00   Min.   :3.182e+06  \n 1st Qu.:0.0000   Class :character   1st Qu.:   12.00   1st Qu.:2.016e+07  \n Median :0.0000   Mode  :character   Median :   18.00   Median :4.808e+07  \n Mean   :0.2033                      Mean   :   64.84   Mean   :2.575e+08  \n 3rd Qu.:0.0000                      3rd Qu.:   35.00   3rd Qu.:1.204e+08  \n Max.   :1.0000                      Max.   :77465.00   Max.   :1.073e+12  \n\n\n\n\nThe dataset is now completely formatted. It includes the RFM_Segment for our Treemap, churn_event for the Kaplan-Meier survival curves, and strictly typed numeric columns for the K-Means clustering and predictive regression models."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "analysis.html",
    "href": "analysis.html",
    "title": "Exploratory & Confirmatory Analysis",
    "section": "",
    "text": "Before building predictive models and advanced macro-visualizations, it is crucial to understand the fundamental distributions and behavioral correlations within our customer base. This phase is divided into Exploratory Data Analysis (EDA) and Confirmatory Data Analysis (CDA).\n\n\n\n\n\nCode\n# Load all required libraries for advanced interactive analytics\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(ggplot2)\nlibrary(plotly)\nlibrary(survival)\nlibrary(networkD3)\nlibrary(scales)\nlibrary(broom) # For tidying survival models\n\n# Load the core application dataset\ndf &lt;- readRDS(\"app_data.rds\")\n\n# Define FinTech Color Palette\nfintech_colors &lt;- c(\"#1c2541\", \"#0077b6\", \"#00b4d8\", \"#90e0ef\", \"#caf0f8\")\nrisk_color &lt;- \"#e74c3c\"\n\n\n\n\n\nOur EDA phase focuses on understanding who our customers are. Using ggplot2 and plotly in our application, stakeholders can dynamically filter these demographic distributions by gender and customer segment.\n\n\nOur EDA phase focuses on understanding who our customers are. We visualized the distribution of customers across education levels to see how age correlates with educational attainment.\n\n\nCode\np_eda &lt;- ggplot(df, aes(x = education_level, y = age, fill = education_level)) +\n  geom_boxplot(alpha = 0.8, color = \"#1c2541\", outlier.color = risk_color) +\n  scale_fill_manual(values = colorRampPalette(fintech_colors)(length(unique(df$education_level)))) +\n  theme_minimal() +\n  labs(title = \"Age Distribution by Education Level\", \n       x = \"Education Level\", \n       y = \"Age\") +\n  theme(legend.position = \"none\", \n        panel.grid.minor = element_blank())\n\n# Convert to interactive Plotly object\nggplotly(p_eda) %&gt;% config(displayModeBar = FALSE)\n\n\n\n\n\n\n\n\n\n\nThe CDA phase seeks to test hypotheses regarding how customer behavior impacts overall business metrics like Customer Lifetime Value (CLV) and Net Promoter Score (NPS).\n\n\nWe deployed interactive scatter plots that allow users to map specific X and Y variables against each other (e.g., Total Transactions vs.¬†Satisfaction Score).\nBy overlaying a statistical trendline (using the lm method), stakeholders can immediately confirm or reject suspected correlations.\n\n\nCode\nggplot(df, aes(x = tx_count, y = satisfaction_score)) +\n  geom_point(alpha = 0.5, color = \"#2c3e50\") +\n  geom_smooth(method = \"lm\", color = \"#e74c3c\", se = FALSE) +\n  theme_minimal() +\n  labs(title = \"Total Transactions vs Satisfaction Score\",\n       x = \"Total Transaction Count\",\n       y = \"Satisfaction Score (1-5)\")\n\n\n\n\n\n\n\n\n\n\n\n\nWe plotted transaction counts against satisfaction scores. The red trendline highlights a concerning friction point: higher transaction volumes do not strictly guarantee higher satisfaction.\n\n\nCode\np_cda &lt;- ggplot(df, aes(x = tx_count, y = satisfaction_score, text = paste(\"Support Tickets:\", support_tickets_count))) +\n  geom_point(alpha = 0.4, color = \"#0077b6\") +\n  geom_smooth(method = \"lm\", color = risk_color, se = TRUE, fill = \"#ffcccb\", alpha = 0.3) +\n  theme_minimal() +\n  labs(title = \"Hypothesis Testing: Usage vs Satisfaction\",\n       x = \"Total Transaction Count\",\n       y = \"Satisfaction Score (1-5)\")\n\nggplotly(p_cda, tooltip = c(\"x\", \"y\", \"text\")) %&gt;% config(displayModeBar = FALSE)\n\n\n\n\n\n\n\n\n\nWe aggregated the total dollar volume flowing through four distinct transactional categories:\n\nPayments\nTransfers\nWithdrawals\nDeposits\n\nThis breakdown acts as the foundational baseline that later informs the proportional weights used in our Macro Cash Flow Sankey Diagram.\n\n\nCode\n# Calculate the summaries using the actual columns in the dataset!\nlibrary(dplyr)\n\ntx_sums &lt;- df %&gt;%\n  group_by(preferred_transaction_type) %&gt;%\n  summarise(\n    Total_Volume = sum(total_tx_volume, na.rm = TRUE)\n  ) %&gt;%\n  arrange(desc(Total_Volume)) # Sorts it from highest to lowest volume\n\n# Print the final data table to the webpage\ntx_sums\n\n\n# A tibble: 4 √ó 2\n  preferred_transaction_type Total_Volume\n  &lt;chr&gt;                             &lt;dbl&gt;\n1 Transfer                        1.01e13\n2 Payment                         2.04e12\n3 Withdrawal                      3.34e11\n4 Deposit                         6.76e10\n\n\n\n\n\n\nTo move beyond basic demographic segmentation, we applied a K-Means Clustering algorithm (k=3) to automatically group users based on their transaction frequency and monetary volume to discover hidden behavioral groupings within our user base. By scaling and analyzing tx_count (Transaction Frequency) against total_tx_volume (Monetary Value), the algorithm identified three distinct customer clusters without any manual labeling.\n\n\nCode\n# 1. Prepare data and apply K-Means\nset.seed(42)\ncluster_data &lt;- df %&gt;% select(tx_count, total_tx_volume) %&gt;% drop_na()\nkmeans_result &lt;- kmeans(scale(cluster_data), centers = 3)\ncluster_data$Cluster &lt;- paste(\"Persona\", kmeans_result$cluster)\n\n# 2. Interactive Cluster Plot\np_cluster &lt;- ggplot(cluster_data, aes(x = tx_count, y = total_tx_volume, color = Cluster)) +\n  geom_point(alpha = 0.6, size = 2) +\n  scale_color_manual(values = c(\"#0077b6\", \"#00b4d8\", risk_color)) +\n  scale_y_continuous(labels = scales::dollar_format()) +\n  theme_minimal() +\n  labs(title = \"Algorithmic Behavioral Segmentation\",\n       x = \"Transaction Frequency\",\n       y = \"Total Monetary Volume\")\n\nggplotly(p_cluster) %&gt;% layout(legend = list(orientation = \"h\", x = 0.3, y = -0.2))\n\n\n\n\n\n\n\n\n\nWith a solid understanding of our demographics and baseline transaction volumes established in this EDA/CDA phase, we confidently move forward to our predictive and advanced modeling phases. The insights gathered here directly feed into the variables chosen for our Linear Regression Risk Simulator and Kaplan-Meier Survival curves.\n\n\nWhile K-Means provides algorithmic grouping, RFM (Recency, Frequency, Monetary) provides established business logic. We categorized our users based on their `R_Score`, `F_Score`, and `M_Score` to identify key groups like ‚ÄúChampions‚Äù and ‚ÄúAt-Risk‚Äù users. In our Shiny application, this is visualized as an interactive Treemap.\n\n\nCode\n## 4. Advanced Segmentation (RFM Analysis)\n#| fig-align: \"center\"\nlibrary(dplyr)\nlibrary(ggplot2)\n\n# Calculate the size of each RFM Segment\nrfm_summary &lt;- df %&gt;%\n  group_by(RFM_Segment) %&gt;%\n  summarise(Customer_Count = n()) %&gt;%\n  arrange(desc(Customer_Count))\n\n# Plotting the Segment Distribution\nggplot(rfm_summary, aes(x = reorder(RFM_Segment, Customer_Count), y = Customer_Count, fill = RFM_Segment)) +\n  geom_bar(stat = \"identity\", alpha = 0.8) +\n  coord_flip() +\n  theme_minimal() +\n  theme(legend.position = \"none\") +\n  labs(title = \"Customer Distribution by RFM Segment\",\n       x = \"RFM Segment\",\n       y = \"Number of Customers\")\n\n\n\n\n\n\n\n\n\n\n\n\nRFM Analysis provides established business logic. Here is the entire distribution of our customer base grouped by their Recency, Frequency, and Monetary scores.\n\n\nCode\n# Calculate RFM Sizes\nrfm_summary &lt;- df %&gt;%\n  group_by(RFM_Segment) %&gt;%\n  summarise(Customer_Count = n()) %&gt;%\n  arrange(desc(Customer_Count))\n\n# Build an interactive Plotly Treemap\nplot_ly(\n  type = \"treemap\",\n  labels = rfm_summary$RFM_Segment,\n  parents = rep(\"All Customers\", nrow(rfm_summary)),\n  values = rfm_summary$Customer_Count,\n  textinfo = \"label+value+percent parent\",\n  marker = list(colorscale = \"Blues\")\n) %&gt;%\n  layout(title = \"Customer Distribution by RFM Segment\")\n\n\n\n\n\n\n\n\n\nTo understand the macro-dynamics of our user base, we explored the dimensions of Time (when users leave) and Capital (where money flows) by using Survival Analysis (Time-to-Churn).\nWe applied a Kaplan-Meier Survival Model to map the probability of a customer remaining active over time. By using customer_tenure and the binary churn_event indicator, we can pinpoint the exact month where retention efforts must be intensified.\n\n\nCode\nlibrary(survival)\n\n# Build the Kaplan-Meier survival object\nkm_fit &lt;- survfit(Surv(customer_tenure, churn_event) ~ 1, data = df)\n\n# Plot the baseline survival curve using base R (survminer is used in the app)\nplot(km_fit, \n     xlab = \"Customer Tenure (Months)\", \n     ylab = \"Survival Probability\", \n     main = \"Kaplan-Meier Estimate of Customer Retention\",\n     col = \"#0077b6\", lwd = 2, conf.int = TRUE)\nabline(v = 3, col = \"red\", lty = 2) # Highlight Month 3 risk zone\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Build Kaplan-Meier Model\nkm_fit &lt;- survfit(Surv(customer_tenure, churn_event) ~ 1, data = df)\nkm_data &lt;- tidy(km_fit) # Use broom to tidy for ggplot\n\n# Interactive Survival Curve\np_surv &lt;- ggplot(km_data, aes(x = time, y = estimate)) +\n  geom_step(color = \"#1c2541\", size = 1) +\n  geom_ribbon(aes(ymin = conf.low, ymax = conf.high), alpha = 0.2, fill = \"#0077b6\") +\n  geom_vline(xintercept = 3, color = risk_color, linetype = \"dashed\") +\n  scale_y_continuous(labels = scales::percent_format()) +\n  theme_minimal() +\n  labs(title = \"Kaplan-Meier Retention Curve\",\n       x = \"Customer Tenure (Months)\",\n       y = \"Probability of Retaining User\") +\n  annotate(\"text\", x = 4.5, y = 0.6, label = \"High Risk Zone\", color = risk_color)\n\nggplotly(p_surv) %&gt;% config(displayModeBar = FALSE)\n\n\n\n\n\n\n\n\n\nTo visualize liquidity, we mapped the flow of transactions using our actual transactional dataset. Deposits represent capital entering the FinRetain ecosystem (‚ÄúApp Wallet‚Äù), while Payments, Transfers, and Withdrawals represent capital flowing out to various destinations.\n\n\nCode\nlibrary(readxl)\n# 1. Load data & summarize\ntx_data &lt;- read_xlsx(\"transactions_data.xlsx\")\ntx_summary &lt;- tx_data %&gt;%\n  group_by(type) %&gt;%\n  summarise(total_volume = sum(amount, na.rm = TRUE))\n\n# 2. Map logical flow\nlinks &lt;- data.frame(\n  source = c(\"External Accounts\", \"App Wallet\", \"App Wallet\", \"App Wallet\"),\n  target = c(\"App Wallet\", \"Merchant Payments\", \"Peer Transfers\", \"External Withdrawals\"),\n  type_name = c(\"Deposit\", \"Payment\", \"Transfer\", \"Withdrawal\")\n) %&gt;%\n  left_join(tx_summary, by = c(\"type_name\" = \"type\")) %&gt;%\n  rename(value = total_volume) %&gt;% filter(value &gt; 0)\n\n# 3. Calculate Node Totals\nnode_totals &lt;- data.frame(name = unique(c(links$source, links$target))) %&gt;%\n  rowwise() %&gt;%\n  mutate(total = max(sum(links$value[links$source == name]), sum(links$value[links$target == name])),\n         label = paste0(name, \" ($\", format(round(total, 0), big.mark = \",\"), \")\"))\n\n# 4. Indices for D3\nlinks$IDsource &lt;- match(links$source, node_totals$name) - 1\nlinks$IDtarget &lt;- match(links$target, node_totals$name) - 1\n\n# 5. Render D3 Interactive Sankey\nsankeyNetwork(Links = links, Nodes = node_totals, Source = \"IDsource\", Target = \"IDtarget\",\n              Value = \"value\", NodeID = \"label\", fontSize = 14, nodeWidth = 40, \n              nodePadding = 20, sinksRight = FALSE, \n              colourScale = JS(\"d3.scaleOrdinal().range(['#1c2541', '#0077b6', '#00b4d8', '#90e0ef']);\"))\n\n\n\n\n\n\n\n\n\nThe core of our Visual Analytics solution is transitioning from reactive data to proactive forecasting. We built a baseline Multiple Linear/Logistic Regression Model to predict churn_probability based on user behavior metrics. This mathematical model powers the ‚ÄúRisk Simulator‚Äù in our Shiny App, allowing stakeholders to use slider inputs to change these variables and instantly recalculate the predicted risk.\n\n\nCode\n# Build a baseline predictive model for Churn Probability\npredictive_model &lt;- lm(churn_probability ~ support_tickets_count + tx_count + satisfaction_score, data = df)\n\n# Display the model summary to confirm statistical significance of variables\nsummary(predictive_model)\n\n\n\nCall:\nlm(formula = churn_probability ~ support_tickets_count + tx_count + \n    satisfaction_score, data = df)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.20780 -0.03801  0.01245  0.04759  0.16247 \n\nCoefficients:\n                        Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)            4.314e-01  2.192e-03 196.817  &lt; 2e-16 ***\nsupport_tickets_count -7.181e-05  2.988e-04  -0.240     0.81    \ntx_count              -2.727e-06  4.881e-07  -5.587 2.32e-08 ***\nsatisfaction_score    -2.119e-02  5.163e-04 -41.037  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.06592 on 48719 degrees of freedom\nMultiple R-squared:  0.03417,   Adjusted R-squared:  0.03411 \nF-statistic: 574.6 on 3 and 48719 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "analysis.html#loading-packages",
    "href": "analysis.html#loading-packages",
    "title": "Exploratory & Confirmatory Analysis",
    "section": "",
    "text": "Code\n# Load all required libraries for advanced interactive analytics\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(ggplot2)\nlibrary(plotly)\nlibrary(survival)\nlibrary(networkD3)\nlibrary(scales)\nlibrary(broom) # For tidying survival models\n\n# Load the core application dataset\ndf &lt;- readRDS(\"app_data.rds\")\n\n# Define FinTech Color Palette\nfintech_colors &lt;- c(\"#1c2541\", \"#0077b6\", \"#00b4d8\", \"#90e0ef\", \"#caf0f8\")\nrisk_color &lt;- \"#e74c3c\""
  },
  {
    "objectID": "analysis.html#demographics-eda",
    "href": "analysis.html#demographics-eda",
    "title": "Exploratory & Confirmatory Analysis",
    "section": "",
    "text": "Our EDA phase focuses on understanding who our customers are. Using ggplot2 and plotly in our application, stakeholders can dynamically filter these demographic distributions by gender and customer segment.\n\n\nOur EDA phase focuses on understanding who our customers are. We visualized the distribution of customers across education levels to see how age correlates with educational attainment.\n\n\nCode\np_eda &lt;- ggplot(df, aes(x = education_level, y = age, fill = education_level)) +\n  geom_boxplot(alpha = 0.8, color = \"#1c2541\", outlier.color = risk_color) +\n  scale_fill_manual(values = colorRampPalette(fintech_colors)(length(unique(df$education_level)))) +\n  theme_minimal() +\n  labs(title = \"Age Distribution by Education Level\", \n       x = \"Education Level\", \n       y = \"Age\") +\n  theme(legend.position = \"none\", \n        panel.grid.minor = element_blank())\n\n# Convert to interactive Plotly object\nggplotly(p_eda) %&gt;% config(displayModeBar = FALSE)"
  },
  {
    "objectID": "analysis.html#behavioral-analysis-cda",
    "href": "analysis.html#behavioral-analysis-cda",
    "title": "Exploratory & Confirmatory Analysis",
    "section": "",
    "text": "The CDA phase seeks to test hypotheses regarding how customer behavior impacts overall business metrics like Customer Lifetime Value (CLV) and Net Promoter Score (NPS).\n\n\nWe deployed interactive scatter plots that allow users to map specific X and Y variables against each other (e.g., Total Transactions vs.¬†Satisfaction Score).\nBy overlaying a statistical trendline (using the lm method), stakeholders can immediately confirm or reject suspected correlations.\n\n\nCode\nggplot(df, aes(x = tx_count, y = satisfaction_score)) +\n  geom_point(alpha = 0.5, color = \"#2c3e50\") +\n  geom_smooth(method = \"lm\", color = \"#e74c3c\", se = FALSE) +\n  theme_minimal() +\n  labs(title = \"Total Transactions vs Satisfaction Score\",\n       x = \"Total Transaction Count\",\n       y = \"Satisfaction Score (1-5)\")\n\n\n\n\n\n\n\n\n\n\n\n\nWe plotted transaction counts against satisfaction scores. The red trendline highlights a concerning friction point: higher transaction volumes do not strictly guarantee higher satisfaction.\n\n\nCode\np_cda &lt;- ggplot(df, aes(x = tx_count, y = satisfaction_score, text = paste(\"Support Tickets:\", support_tickets_count))) +\n  geom_point(alpha = 0.4, color = \"#0077b6\") +\n  geom_smooth(method = \"lm\", color = risk_color, se = TRUE, fill = \"#ffcccb\", alpha = 0.3) +\n  theme_minimal() +\n  labs(title = \"Hypothesis Testing: Usage vs Satisfaction\",\n       x = \"Total Transaction Count\",\n       y = \"Satisfaction Score (1-5)\")\n\nggplotly(p_cda, tooltip = c(\"x\", \"y\", \"text\")) %&gt;% config(displayModeBar = FALSE)\n\n\n\n\n\n\n\n\n\nWe aggregated the total dollar volume flowing through four distinct transactional categories:\n\nPayments\nTransfers\nWithdrawals\nDeposits\n\nThis breakdown acts as the foundational baseline that later informs the proportional weights used in our Macro Cash Flow Sankey Diagram.\n\n\nCode\n# Calculate the summaries using the actual columns in the dataset!\nlibrary(dplyr)\n\ntx_sums &lt;- df %&gt;%\n  group_by(preferred_transaction_type) %&gt;%\n  summarise(\n    Total_Volume = sum(total_tx_volume, na.rm = TRUE)\n  ) %&gt;%\n  arrange(desc(Total_Volume)) # Sorts it from highest to lowest volume\n\n# Print the final data table to the webpage\ntx_sums\n\n\n# A tibble: 4 √ó 2\n  preferred_transaction_type Total_Volume\n  &lt;chr&gt;                             &lt;dbl&gt;\n1 Transfer                        1.01e13\n2 Payment                         2.04e12\n3 Withdrawal                      3.34e11\n4 Deposit                         6.76e10"
  },
  {
    "objectID": "analysis.html#cluster-analysis",
    "href": "analysis.html#cluster-analysis",
    "title": "Exploratory & Confirmatory Analysis",
    "section": "",
    "text": "To move beyond basic demographic segmentation, we applied a K-Means Clustering algorithm (k=3) to automatically group users based on their transaction frequency and monetary volume to discover hidden behavioral groupings within our user base. By scaling and analyzing tx_count (Transaction Frequency) against total_tx_volume (Monetary Value), the algorithm identified three distinct customer clusters without any manual labeling.\n\n\nCode\n# 1. Prepare data and apply K-Means\nset.seed(42)\ncluster_data &lt;- df %&gt;% select(tx_count, total_tx_volume) %&gt;% drop_na()\nkmeans_result &lt;- kmeans(scale(cluster_data), centers = 3)\ncluster_data$Cluster &lt;- paste(\"Persona\", kmeans_result$cluster)\n\n# 2. Interactive Cluster Plot\np_cluster &lt;- ggplot(cluster_data, aes(x = tx_count, y = total_tx_volume, color = Cluster)) +\n  geom_point(alpha = 0.6, size = 2) +\n  scale_color_manual(values = c(\"#0077b6\", \"#00b4d8\", risk_color)) +\n  scale_y_continuous(labels = scales::dollar_format()) +\n  theme_minimal() +\n  labs(title = \"Algorithmic Behavioral Segmentation\",\n       x = \"Transaction Frequency\",\n       y = \"Total Monetary Volume\")\n\nggplotly(p_cluster) %&gt;% layout(legend = list(orientation = \"h\", x = 0.3, y = -0.2))"
  },
  {
    "objectID": "analysis.html#transition-to-advanced-analytics",
    "href": "analysis.html#transition-to-advanced-analytics",
    "title": "Exploratory & Confirmatory Analysis",
    "section": "",
    "text": "With a solid understanding of our demographics and baseline transaction volumes established in this EDA/CDA phase, we confidently move forward to our predictive and advanced modeling phases. The insights gathered here directly feed into the variables chosen for our Linear Regression Risk Simulator and Kaplan-Meier Survival curves.\n\n\nWhile K-Means provides algorithmic grouping, RFM (Recency, Frequency, Monetary) provides established business logic. We categorized our users based on their `R_Score`, `F_Score`, and `M_Score` to identify key groups like ‚ÄúChampions‚Äù and ‚ÄúAt-Risk‚Äù users. In our Shiny application, this is visualized as an interactive Treemap.\n\n\nCode\n## 4. Advanced Segmentation (RFM Analysis)\n#| fig-align: \"center\"\nlibrary(dplyr)\nlibrary(ggplot2)\n\n# Calculate the size of each RFM Segment\nrfm_summary &lt;- df %&gt;%\n  group_by(RFM_Segment) %&gt;%\n  summarise(Customer_Count = n()) %&gt;%\n  arrange(desc(Customer_Count))\n\n# Plotting the Segment Distribution\nggplot(rfm_summary, aes(x = reorder(RFM_Segment, Customer_Count), y = Customer_Count, fill = RFM_Segment)) +\n  geom_bar(stat = \"identity\", alpha = 0.8) +\n  coord_flip() +\n  theme_minimal() +\n  theme(legend.position = \"none\") +\n  labs(title = \"Customer Distribution by RFM Segment\",\n       x = \"RFM Segment\",\n       y = \"Number of Customers\")\n\n\n\n\n\n\n\n\n\n\n\n\nRFM Analysis provides established business logic. Here is the entire distribution of our customer base grouped by their Recency, Frequency, and Monetary scores.\n\n\nCode\n# Calculate RFM Sizes\nrfm_summary &lt;- df %&gt;%\n  group_by(RFM_Segment) %&gt;%\n  summarise(Customer_Count = n()) %&gt;%\n  arrange(desc(Customer_Count))\n\n# Build an interactive Plotly Treemap\nplot_ly(\n  type = \"treemap\",\n  labels = rfm_summary$RFM_Segment,\n  parents = rep(\"All Customers\", nrow(rfm_summary)),\n  values = rfm_summary$Customer_Count,\n  textinfo = \"label+value+percent parent\",\n  marker = list(colorscale = \"Blues\")\n) %&gt;%\n  layout(title = \"Customer Distribution by RFM Segment\")\n\n\n\n\n\n\n\n\n\nTo understand the macro-dynamics of our user base, we explored the dimensions of Time (when users leave) and Capital (where money flows) by using Survival Analysis (Time-to-Churn).\nWe applied a Kaplan-Meier Survival Model to map the probability of a customer remaining active over time. By using customer_tenure and the binary churn_event indicator, we can pinpoint the exact month where retention efforts must be intensified.\n\n\nCode\nlibrary(survival)\n\n# Build the Kaplan-Meier survival object\nkm_fit &lt;- survfit(Surv(customer_tenure, churn_event) ~ 1, data = df)\n\n# Plot the baseline survival curve using base R (survminer is used in the app)\nplot(km_fit, \n     xlab = \"Customer Tenure (Months)\", \n     ylab = \"Survival Probability\", \n     main = \"Kaplan-Meier Estimate of Customer Retention\",\n     col = \"#0077b6\", lwd = 2, conf.int = TRUE)\nabline(v = 3, col = \"red\", lty = 2) # Highlight Month 3 risk zone\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Build Kaplan-Meier Model\nkm_fit &lt;- survfit(Surv(customer_tenure, churn_event) ~ 1, data = df)\nkm_data &lt;- tidy(km_fit) # Use broom to tidy for ggplot\n\n# Interactive Survival Curve\np_surv &lt;- ggplot(km_data, aes(x = time, y = estimate)) +\n  geom_step(color = \"#1c2541\", size = 1) +\n  geom_ribbon(aes(ymin = conf.low, ymax = conf.high), alpha = 0.2, fill = \"#0077b6\") +\n  geom_vline(xintercept = 3, color = risk_color, linetype = \"dashed\") +\n  scale_y_continuous(labels = scales::percent_format()) +\n  theme_minimal() +\n  labs(title = \"Kaplan-Meier Retention Curve\",\n       x = \"Customer Tenure (Months)\",\n       y = \"Probability of Retaining User\") +\n  annotate(\"text\", x = 4.5, y = 0.6, label = \"High Risk Zone\", color = risk_color)\n\nggplotly(p_surv) %&gt;% config(displayModeBar = FALSE)\n\n\n\n\n\n\n\n\n\nTo visualize liquidity, we mapped the flow of transactions using our actual transactional dataset. Deposits represent capital entering the FinRetain ecosystem (‚ÄúApp Wallet‚Äù), while Payments, Transfers, and Withdrawals represent capital flowing out to various destinations.\n\n\nCode\nlibrary(readxl)\n# 1. Load data & summarize\ntx_data &lt;- read_xlsx(\"transactions_data.xlsx\")\ntx_summary &lt;- tx_data %&gt;%\n  group_by(type) %&gt;%\n  summarise(total_volume = sum(amount, na.rm = TRUE))\n\n# 2. Map logical flow\nlinks &lt;- data.frame(\n  source = c(\"External Accounts\", \"App Wallet\", \"App Wallet\", \"App Wallet\"),\n  target = c(\"App Wallet\", \"Merchant Payments\", \"Peer Transfers\", \"External Withdrawals\"),\n  type_name = c(\"Deposit\", \"Payment\", \"Transfer\", \"Withdrawal\")\n) %&gt;%\n  left_join(tx_summary, by = c(\"type_name\" = \"type\")) %&gt;%\n  rename(value = total_volume) %&gt;% filter(value &gt; 0)\n\n# 3. Calculate Node Totals\nnode_totals &lt;- data.frame(name = unique(c(links$source, links$target))) %&gt;%\n  rowwise() %&gt;%\n  mutate(total = max(sum(links$value[links$source == name]), sum(links$value[links$target == name])),\n         label = paste0(name, \" ($\", format(round(total, 0), big.mark = \",\"), \")\"))\n\n# 4. Indices for D3\nlinks$IDsource &lt;- match(links$source, node_totals$name) - 1\nlinks$IDtarget &lt;- match(links$target, node_totals$name) - 1\n\n# 5. Render D3 Interactive Sankey\nsankeyNetwork(Links = links, Nodes = node_totals, Source = \"IDsource\", Target = \"IDtarget\",\n              Value = \"value\", NodeID = \"label\", fontSize = 14, nodeWidth = 40, \n              nodePadding = 20, sinksRight = FALSE, \n              colourScale = JS(\"d3.scaleOrdinal().range(['#1c2541', '#0077b6', '#00b4d8', '#90e0ef']);\"))\n\n\n\n\n\n\n\n\n\nThe core of our Visual Analytics solution is transitioning from reactive data to proactive forecasting. We built a baseline Multiple Linear/Logistic Regression Model to predict churn_probability based on user behavior metrics. This mathematical model powers the ‚ÄúRisk Simulator‚Äù in our Shiny App, allowing stakeholders to use slider inputs to change these variables and instantly recalculate the predicted risk.\n\n\nCode\n# Build a baseline predictive model for Churn Probability\npredictive_model &lt;- lm(churn_probability ~ support_tickets_count + tx_count + satisfaction_score, data = df)\n\n# Display the model summary to confirm statistical significance of variables\nsummary(predictive_model)\n\n\n\nCall:\nlm(formula = churn_probability ~ support_tickets_count + tx_count + \n    satisfaction_score, data = df)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.20780 -0.03801  0.01245  0.04759  0.16247 \n\nCoefficients:\n                        Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)            4.314e-01  2.192e-03 196.817  &lt; 2e-16 ***\nsupport_tickets_count -7.181e-05  2.988e-04  -0.240     0.81    \ntx_count              -2.727e-06  4.881e-07  -5.587 2.32e-08 ***\nsatisfaction_score    -2.119e-02  5.163e-04 -41.037  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.06592 on 48719 degrees of freedom\nMultiple R-squared:  0.03417,   Adjusted R-squared:  0.03411 \nF-statistic: 574.6 on 3 and 48719 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "findings.html",
    "href": "findings.html",
    "title": "Findings, Results & Discussion",
    "section": "",
    "text": "The FinRetain visual analytics project set out to democratize complex transactional data and uncover the hidden drivers of customer churn. Through a rigorous pipeline of Exploratory Data Analysis, Algorithmic Clustering, Macro-Ecosystem Mapping, and Predictive Modelling, we have transitioned our understanding of the user base from reactive observation to proactive forecasting. Below are the synthesized findings and strategic business recommendations derived from our interactive dashboard.\n\n\n\nOur initial hypothesis assumed that users with the highest transaction volumes would inherently possess the highest satisfaction scores. Our Confirmatory Data Analysis (CDA) proved this false.\n\nVolume Distribution: The transaction breakdown indicates that Direct Deposits are the primary driver of incoming liquidity, while Payments (rather than Cash Withdrawals) constitute the highest volume of outgoing capital.\nIncome vs.¬†App Engagement: Customers in the ‚ÄúHigh‚Äù and ‚ÄúVery High‚Äù income brackets exhibit significantly higher transaction frequencies. However, they are also highly sensitive to friction, showing a steep drop in Satisfaction Score when their support_tickets_count increases.\nThe Finding: While high-income and highly educated users transact more frequently, their satisfaction scores exhibit high variance. By mapping support_tickets_count onto our scatter plots, a clear negative correlation emerged.\nThe Insight: The FinRetain App‚Äôs core UX is functional, but our customer support pipeline is a severe bottleneck. High-value users lose faith in the platform the moment they encounter a dispute that requires a support ticket.\nActionable Step: Implement automated, AI-driven ticket resolution for ‚ÄúHigh-Value‚Äù users to reduce friction and protect satisfaction scores.\n\n\n\n\n\nManual demographic filtering is insufficient for modern FinTech marketing. We deployed both K-Means clustering and RFM (Recency, Frequency, Monetary) analysis to segment our users. Rather than relying solely on demographic assumptions, we deployed algorithmic clustering to find hidden behavioral patterns, and the dynamic RFM (Recency, Frequency, Monetary) Treemap allowed us to instantly categorize our entire database into actionable blocks:\n\nThree Personas: By running K-Means on tx_count and total_tx_volume, the algorithm autonomously grouped our users into three distinct personas. It highlighted a massive ‚Äúmiddle-class‚Äù of users who log in and transact frequently, but move very little actual monetary value.\nK-Means Findings: The algorithm naturally grouped our users into three distinct behavioral personas based purely on transaction frequency and monetary volume. We identified a massive ‚Äúmiddle-class‚Äù of users who transact frequently but with very low monetary value.\nChampions & Loyalists: While making up a smaller physical percentage of the user base, the dark color mapping (indicating high Customer Lifetime Value) confirmed that this group drives the vast majority of future projected revenue.\nAt-Risk Segments: A surprisingly large block of users fell into the ‚ÄúRecent Users‚Äù category with dangerously low Frequency scores. These users are at immediate risk of churning if not engaged quickly.\nRFM Findings: Our interactive Treemap revealed that while we have a solid block of ‚ÄúChampions‚Äù (High R, F, and M scores), an alarming percentage of users fall into the ‚ÄúRecent but Low Frequency‚Äù block.\nActionable Step: Gamify the daily app experience (e.g., daily login rewards, micro-investing features) specifically targeted at the low-frequency segments identified in the Treemap to convert them into Champions.\n\n\n\n\n\nUnderstanding who leaves is only half the battle; we needed to know when they leave and where the money goes. Using the Kaplan-Meier Churn Probability Curve, we plotted the exact tenure months where different customer segments are statistically most likely to abandon the app.\nThe Critical Drop-Off Window\nWe observed a steep drop in the survival probability curve between Month 3 and Month 6 across all acquisition channels. If a customer survives past Month 6, their probability of long-term retention increases exponentially.\nChannel Variance\nCustomers acquired via specific marketing channels showed inherently different survival trajectories, suggesting that the quality of onboarding greatly impacts long-term loyalty.\n\n\n\nThe Finding: Our Kaplan-Meier Survival Curve pinpointed a critical ‚ÄúDanger Zone‚Äù between Month 3 and Month 6 of a customer‚Äôs tenure. The probability of survival plummets drastically during this 90-day window.\nThe Insight: Users are downloading the app, trying it for a few months, and abandoning it if it doesn‚Äôt become their primary financial hub.\nActionable Step: Marketing must deploy aggressive, targeted retention campaigns (e.g., fee waivers, premium account trials) exactly at the 60-day mark to preempt the Month 3 drop-off.\n\n\n\n\n\nThe Finding: The Macro Cash Flow diagram mapped the exact dollar amounts flowing through the ecosystem. It visually proved that Direct Deposits are the absolute lifeblood of the platform, but a significant portion of that capital immediately flows out via External Withdrawals rather than staying in the app ecosystem (like Peer Transfers or App Investments).\nActionable Step: Incentivize users to keep their money inside the FinRetain ecosystem by offering higher yield savings rates for capital that originates from Direct Deposits.\n\n\n\n\n\n\nThe network analysis of capital flow mapped exactly how money enters and leaves the FinRetain ecosystem.\nCapital Retention: By balancing the visual nodes, we identified the exact proportion of incoming funds that are immediately drained via ‚ÄúExternal Withdrawals‚Äù versus funds that are kept within the ecosystem for ‚ÄúMerchant Payments‚Äù and ‚ÄúPeer Transfers.‚Äù\nLiquidity Insights: The thickest flow lines originate from external Deposits, proving that convincing a user to set up direct payroll deposit is the single most valuable action the app can incentivize.\n\n\n\n\nThe most significant achievement of the FinRetain project is the Risk Simulator. By integrating a Multiple Linear Regression model into our Shiny UI, we proved that churn is not inevitable. The model confirmed that support_tickets_count and tx_count are statistically significant predictors of churn_probability. Our multiple linear regression model confirmed that user behavior directly dictates future value.\n\nSimulated Salvage: By interacting with the UI sliders, we proved that artificially lowering a user‚Äôs support ticket count and slightly increasing their transaction frequency drastically shifts their predicted Churn Probability from ‚ÄúHigh Risk‚Äù (Red) to ‚ÄúSafe‚Äù (Green), effectively saving their Customer Lifetime Value (CLV).\nThe Result: Stakeholders can now use the dashboard sliders to simulate interventions. If a user has an 80% churn risk, the stakeholder can simulate lowering their support tickets (via faster customer service) and increasing their transaction frequency (via marketing pushes). The visual gauges update instantly, showing how the user‚Äôs Customer Lifetime Value (CLV) is salvaged.\n\n\n\n\n\nBased on the quantitative results derived from the FinRetain platform, our team proposes the following strategic business interventions:\n\n\nSince the Survival Analysis proved that Months 3 through 6 are the most dangerous for customer churn, the marketing team should automate targeted retention campaigns (e.g., fee waivers, premium feature trials) specifically triggered at the 90-day tenure mark.\n\n\n\nThe RFM Treemap highlighted a massive block of low-frequency ‚ÄúRecent Users.‚Äù We recommend using the insights from the Risk Simulator module to gamify the app experience (via daily login rewards or micro-investing features) to artificially inflate their transaction frequency and protect their projected CLV.\n\n\n\nThe Sankey Cash Flow diagram proved that external Deposits feed the most capital into the app ecosystem. Business development should focus on campaigns that financially reward users for switching their primary salary deposit to FinRetain, as this ensures long-term liquidity.\n\n\n\nOur CDA scatter plots proved that high-income users churn rapidly when dealing with poor customer support. We strongly recommend routing users identified as ‚ÄúChampions‚Äù by our RFM model to a priority, zero-wait-time customer service queue.\n\n\n\n\n\nData visualization is only valuable if it drives decisions. The FinRetain Visual Analytics platform successfully bridges the gap between raw data and executive decision-making. By leveraging R Shiny, plotly, survival models, networkD3, survival curves and networkD3, we have provided stakeholders with an interactive environment and exact insights needed to reduce churn, optimize capital flow, and maximize Customer Lifetime Value to not only view historical demographic data, but to actively simulate and predict future customer behavior."
  },
  {
    "objectID": "findings.html#the-engagement-friction-point-eda-cda-results",
    "href": "findings.html#the-engagement-friction-point-eda-cda-results",
    "title": "Findings, Results & Discussion",
    "section": "",
    "text": "Our initial hypothesis assumed that users with the highest transaction volumes would inherently possess the highest satisfaction scores. Our Confirmatory Data Analysis (CDA) proved this false.\n\nVolume Distribution: The transaction breakdown indicates that Direct Deposits are the primary driver of incoming liquidity, while Payments (rather than Cash Withdrawals) constitute the highest volume of outgoing capital.\nIncome vs.¬†App Engagement: Customers in the ‚ÄúHigh‚Äù and ‚ÄúVery High‚Äù income brackets exhibit significantly higher transaction frequencies. However, they are also highly sensitive to friction, showing a steep drop in Satisfaction Score when their support_tickets_count increases.\nThe Finding: While high-income and highly educated users transact more frequently, their satisfaction scores exhibit high variance. By mapping support_tickets_count onto our scatter plots, a clear negative correlation emerged.\nThe Insight: The FinRetain App‚Äôs core UX is functional, but our customer support pipeline is a severe bottleneck. High-value users lose faith in the platform the moment they encounter a dispute that requires a support ticket.\nActionable Step: Implement automated, AI-driven ticket resolution for ‚ÄúHigh-Value‚Äù users to reduce friction and protect satisfaction scores."
  },
  {
    "objectID": "findings.html#algorithmic-and-rule-based-personas-and-value-satisfaction-segmentation",
    "href": "findings.html#algorithmic-and-rule-based-personas-and-value-satisfaction-segmentation",
    "title": "Findings, Results & Discussion",
    "section": "",
    "text": "Manual demographic filtering is insufficient for modern FinTech marketing. We deployed both K-Means clustering and RFM (Recency, Frequency, Monetary) analysis to segment our users. Rather than relying solely on demographic assumptions, we deployed algorithmic clustering to find hidden behavioral patterns, and the dynamic RFM (Recency, Frequency, Monetary) Treemap allowed us to instantly categorize our entire database into actionable blocks:\n\nThree Personas: By running K-Means on tx_count and total_tx_volume, the algorithm autonomously grouped our users into three distinct personas. It highlighted a massive ‚Äúmiddle-class‚Äù of users who log in and transact frequently, but move very little actual monetary value.\nK-Means Findings: The algorithm naturally grouped our users into three distinct behavioral personas based purely on transaction frequency and monetary volume. We identified a massive ‚Äúmiddle-class‚Äù of users who transact frequently but with very low monetary value.\nChampions & Loyalists: While making up a smaller physical percentage of the user base, the dark color mapping (indicating high Customer Lifetime Value) confirmed that this group drives the vast majority of future projected revenue.\nAt-Risk Segments: A surprisingly large block of users fell into the ‚ÄúRecent Users‚Äù category with dangerously low Frequency scores. These users are at immediate risk of churning if not engaged quickly.\nRFM Findings: Our interactive Treemap revealed that while we have a solid block of ‚ÄúChampions‚Äù (High R, F, and M scores), an alarming percentage of users fall into the ‚ÄúRecent but Low Frequency‚Äù block.\nActionable Step: Gamify the daily app experience (e.g., daily login rewards, micro-investing features) specifically targeted at the low-frequency segments identified in the Treemap to convert them into Champions."
  },
  {
    "objectID": "findings.html#the-churn-timeline-and-capital-bleed",
    "href": "findings.html#the-churn-timeline-and-capital-bleed",
    "title": "Findings, Results & Discussion",
    "section": "",
    "text": "Understanding who leaves is only half the battle; we needed to know when they leave and where the money goes. Using the Kaplan-Meier Churn Probability Curve, we plotted the exact tenure months where different customer segments are statistically most likely to abandon the app.\nThe Critical Drop-Off Window\nWe observed a steep drop in the survival probability curve between Month 3 and Month 6 across all acquisition channels. If a customer survives past Month 6, their probability of long-term retention increases exponentially.\nChannel Variance\nCustomers acquired via specific marketing channels showed inherently different survival trajectories, suggesting that the quality of onboarding greatly impacts long-term loyalty.\n\n\n\nThe Finding: Our Kaplan-Meier Survival Curve pinpointed a critical ‚ÄúDanger Zone‚Äù between Month 3 and Month 6 of a customer‚Äôs tenure. The probability of survival plummets drastically during this 90-day window.\nThe Insight: Users are downloading the app, trying it for a few months, and abandoning it if it doesn‚Äôt become their primary financial hub.\nActionable Step: Marketing must deploy aggressive, targeted retention campaigns (e.g., fee waivers, premium account trials) exactly at the 60-day mark to preempt the Month 3 drop-off.\n\n\n\n\n\nThe Finding: The Macro Cash Flow diagram mapped the exact dollar amounts flowing through the ecosystem. It visually proved that Direct Deposits are the absolute lifeblood of the platform, but a significant portion of that capital immediately flows out via External Withdrawals rather than staying in the app ecosystem (like Peer Transfers or App Investments).\nActionable Step: Incentivize users to keep their money inside the FinRetain ecosystem by offering higher yield savings rates for capital that originates from Direct Deposits."
  },
  {
    "objectID": "findings.html#macro-ecosystem-cash-flow-sankey",
    "href": "findings.html#macro-ecosystem-cash-flow-sankey",
    "title": "Findings, Results & Discussion",
    "section": "",
    "text": "The network analysis of capital flow mapped exactly how money enters and leaves the FinRetain ecosystem.\nCapital Retention: By balancing the visual nodes, we identified the exact proportion of incoming funds that are immediately drained via ‚ÄúExternal Withdrawals‚Äù versus funds that are kept within the ecosystem for ‚ÄúMerchant Payments‚Äù and ‚ÄúPeer Transfers.‚Äù\nLiquidity Insights: The thickest flow lines originate from external Deposits, proving that convincing a user to set up direct payroll deposit is the single most valuable action the app can incentivize."
  },
  {
    "objectID": "findings.html#proactive-intervention-predictive-modelling",
    "href": "findings.html#proactive-intervention-predictive-modelling",
    "title": "Findings, Results & Discussion",
    "section": "",
    "text": "The most significant achievement of the FinRetain project is the Risk Simulator. By integrating a Multiple Linear Regression model into our Shiny UI, we proved that churn is not inevitable. The model confirmed that support_tickets_count and tx_count are statistically significant predictors of churn_probability. Our multiple linear regression model confirmed that user behavior directly dictates future value.\n\nSimulated Salvage: By interacting with the UI sliders, we proved that artificially lowering a user‚Äôs support ticket count and slightly increasing their transaction frequency drastically shifts their predicted Churn Probability from ‚ÄúHigh Risk‚Äù (Red) to ‚ÄúSafe‚Äù (Green), effectively saving their Customer Lifetime Value (CLV).\nThe Result: Stakeholders can now use the dashboard sliders to simulate interventions. If a user has an 80% churn risk, the stakeholder can simulate lowering their support tickets (via faster customer service) and increasing their transaction frequency (via marketing pushes). The visual gauges update instantly, showing how the user‚Äôs Customer Lifetime Value (CLV) is salvaged."
  },
  {
    "objectID": "findings.html#strategic-discussions-recommendations",
    "href": "findings.html#strategic-discussions-recommendations",
    "title": "Findings, Results & Discussion",
    "section": "",
    "text": "Based on the quantitative results derived from the FinRetain platform, our team proposes the following strategic business interventions:\n\n\nSince the Survival Analysis proved that Months 3 through 6 are the most dangerous for customer churn, the marketing team should automate targeted retention campaigns (e.g., fee waivers, premium feature trials) specifically triggered at the 90-day tenure mark.\n\n\n\nThe RFM Treemap highlighted a massive block of low-frequency ‚ÄúRecent Users.‚Äù We recommend using the insights from the Risk Simulator module to gamify the app experience (via daily login rewards or micro-investing features) to artificially inflate their transaction frequency and protect their projected CLV.\n\n\n\nThe Sankey Cash Flow diagram proved that external Deposits feed the most capital into the app ecosystem. Business development should focus on campaigns that financially reward users for switching their primary salary deposit to FinRetain, as this ensures long-term liquidity.\n\n\n\nOur CDA scatter plots proved that high-income users churn rapidly when dealing with poor customer support. We strongly recommend routing users identified as ‚ÄúChampions‚Äù by our RFM model to a priority, zero-wait-time customer service queue."
  },
  {
    "objectID": "findings.html#conclusion",
    "href": "findings.html#conclusion",
    "title": "Findings, Results & Discussion",
    "section": "",
    "text": "Data visualization is only valuable if it drives decisions. The FinRetain Visual Analytics platform successfully bridges the gap between raw data and executive decision-making. By leveraging R Shiny, plotly, survival models, networkD3, survival curves and networkD3, we have provided stakeholders with an interactive environment and exact insights needed to reduce churn, optimize capital flow, and maximize Customer Lifetime Value to not only view historical demographic data, but to actively simulate and predict future customer behavior."
  },
  {
    "objectID": "meeting.html",
    "href": "meeting.html",
    "title": "Project Minutes of Meeting",
    "section": "",
    "text": "Below is the official record of team meetings, key decisions, and technical troubleshooting sessions conducted during the development of the FinRetain application.\n\n\n\nDate: 21/02/2026\nTime: 16:00 - 18:00\nLocation: Campus Library\nAttendees: LIN XIANWEI, SU HUI\n\n\n\nDefine project scope and core analytical modules.\nDiscuss data handling for the large transactions_data file.\nAssign initial roles.\n\n\n\n\n\nData Pipeline: The team identified that the transaction dataset is too large to process live on a free shinyapps.io server.\nDecision: We will build an offline data pipeline to aggregate the transaction types and join them with the customer demographics, saving the output as a highly compressed app_data.rds file for lightning-fast app loading.\nCore Modules: Agreed to build an EDA Demographic tab, a CDA Behavioral tab, and a Churn/CLV Risk Simulator using Linear Regression models.\n\n\n\n\n\n\n\n\n\n\n\n\n\nTask\nAssignee\nDeadline\nStatus\n\n\n\n\nCreate offline data wrangling script (dplyr, tidyr)\nLIN XIANWEI\n21/02/2026\nCompleted\n\n\nBuild base UI shell using bslib\nLIN XIANWEI, SU HUI\n25/02/2026\nCompleted\n\n\nTrain lm predictive models for Simulator\nLIN XIANWEI, SU HUI\n27/02/2026\nCompleted\n\n\n\n\n\n\n\n\nDate: 01/03/2026\nTime: 15:00 - 18:00\nLocation: Via Zoom\nAttendees: LIN XIANWEI, SU HUI\n\n\n\nReview the base Shiny app.\nBrainstorm unique insights to secure high marks for novelty and methodology.\n\n\n\n\n\nNovelty Requirement: Standard bar charts are insufficient for top marks. We need advanced financial analytics.\nDecision 1 (Survival Analysis): We will implement Kaplan-Meier survival curves using the survival package to predict the exact tenure month customers are most likely to churn.\nDecision 2 (RFM Segmentation): We will calculate Recency, Frequency, and Monetary scores on the fly and visualize them using a plotly Treemap.\nDecision 3 (Cash Flow): We will implement a networkD3 Sankey diagram to visualize macro fund flows.\n\n\n\n\n\n\n\n\n\n\n\n\n\nTask\nAssignee\nDeadline\nStatus\n\n\n\n\nWrite dynamic RFM calculation code in app.R\nLIN XIANWEI, SU HUI\n03/03/2026\nCompleted\n\n\nBuild Sankey Diagram logic\nLIN XIANWEI, SU HUI\n09/03/2026\nCompleted\n\n\nImplement Kaplan-Meier plots\nLIN XIANWEI, SU HUI\n14/03/2026\nCompleted\n\n\n\n\n\n\n\n\nDate: 15/03/2026\nTime: 14:00 - 17:00\nLocation: Via Zoom\nAttendees: LIN XIANWEI, SU HUI\n\n\n\nDeploy the unified app.R to shinyapps.io.\nTroubleshoot server crashes and UI bugs.\n\n\n\n\n\nIssue 1: Server Crash on Deployment (Exit Status 1). The server crashed because it was looking for the raw CSV files which were excluded from the deployment folder.\n\nFix: Stripped the offline data pipeline from the deployment version of app.R and loaded app_data.rds directly.\n\nIssue 2: Survival Plot Error (object of type 'symbol' is not subsettable). The ggsurvplot function failed in the reactive Shiny environment.\n\nFix: Replaced survfit() with surv_fit() and explicitly wrapped the plot in a print() command.\n\nIssue 3: Sankey Diagram Visual Imbalance. The left side of the Sankey diagram was visually squashed because incoming funds were much smaller than outgoing volume.\n\nFix: Decoupled the visual drawing weights from the actual dollar amounts. Forced balanced proportions (100% vs 100%) while formatting the text labels to still display the actual, accurate currency amounts ($X,XXX,XXX).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTask\nAssignee\nDeadline\nStatus\n\n\n\n\nFinal code cleanup and commenting\nLIN XIANWEI, SU HUI\n29/03/2026\nCompleted\n\n\nDeploy stable version to Shinyapps.io\nLIN XIANWEI, SU HUI\n20/03/2026\nCompleted\n\n\nWrite project proposal and Quarto site\nLIN XIANWEI, SU HUI\n15/03/2026\nCompleted\n\n\n\n\nMinutes prepared by: TEAM 15 - LIN XIANWEI, SU HUI"
  },
  {
    "objectID": "meeting.html#meeting-1-project-kickoff-data-strategy",
    "href": "meeting.html#meeting-1-project-kickoff-data-strategy",
    "title": "Project Minutes of Meeting",
    "section": "",
    "text": "Date: 21/02/2026\nTime: 16:00 - 18:00\nLocation: Campus Library\nAttendees: LIN XIANWEI, SU HUI\n\n\n\nDefine project scope and core analytical modules.\nDiscuss data handling for the large transactions_data file.\nAssign initial roles.\n\n\n\n\n\nData Pipeline: The team identified that the transaction dataset is too large to process live on a free shinyapps.io server.\nDecision: We will build an offline data pipeline to aggregate the transaction types and join them with the customer demographics, saving the output as a highly compressed app_data.rds file for lightning-fast app loading.\nCore Modules: Agreed to build an EDA Demographic tab, a CDA Behavioral tab, and a Churn/CLV Risk Simulator using Linear Regression models.\n\n\n\n\n\n\n\n\n\n\n\n\n\nTask\nAssignee\nDeadline\nStatus\n\n\n\n\nCreate offline data wrangling script (dplyr, tidyr)\nLIN XIANWEI\n21/02/2026\nCompleted\n\n\nBuild base UI shell using bslib\nLIN XIANWEI, SU HUI\n25/02/2026\nCompleted\n\n\nTrain lm predictive models for Simulator\nLIN XIANWEI, SU HUI\n27/02/2026\nCompleted"
  },
  {
    "objectID": "meeting.html#meeting-2-advanced-analytics-integration",
    "href": "meeting.html#meeting-2-advanced-analytics-integration",
    "title": "Project Minutes of Meeting",
    "section": "",
    "text": "Date: 01/03/2026\nTime: 15:00 - 18:00\nLocation: Via Zoom\nAttendees: LIN XIANWEI, SU HUI\n\n\n\nReview the base Shiny app.\nBrainstorm unique insights to secure high marks for novelty and methodology.\n\n\n\n\n\nNovelty Requirement: Standard bar charts are insufficient for top marks. We need advanced financial analytics.\nDecision 1 (Survival Analysis): We will implement Kaplan-Meier survival curves using the survival package to predict the exact tenure month customers are most likely to churn.\nDecision 2 (RFM Segmentation): We will calculate Recency, Frequency, and Monetary scores on the fly and visualize them using a plotly Treemap.\nDecision 3 (Cash Flow): We will implement a networkD3 Sankey diagram to visualize macro fund flows.\n\n\n\n\n\n\n\n\n\n\n\n\n\nTask\nAssignee\nDeadline\nStatus\n\n\n\n\nWrite dynamic RFM calculation code in app.R\nLIN XIANWEI, SU HUI\n03/03/2026\nCompleted\n\n\nBuild Sankey Diagram logic\nLIN XIANWEI, SU HUI\n09/03/2026\nCompleted\n\n\nImplement Kaplan-Meier plots\nLIN XIANWEI, SU HUI\n14/03/2026\nCompleted"
  },
  {
    "objectID": "meeting.html#meeting-3-deployment-bug-squashing",
    "href": "meeting.html#meeting-3-deployment-bug-squashing",
    "title": "Project Minutes of Meeting",
    "section": "",
    "text": "Date: 15/03/2026\nTime: 14:00 - 17:00\nLocation: Via Zoom\nAttendees: LIN XIANWEI, SU HUI\n\n\n\nDeploy the unified app.R to shinyapps.io.\nTroubleshoot server crashes and UI bugs.\n\n\n\n\n\nIssue 1: Server Crash on Deployment (Exit Status 1). The server crashed because it was looking for the raw CSV files which were excluded from the deployment folder.\n\nFix: Stripped the offline data pipeline from the deployment version of app.R and loaded app_data.rds directly.\n\nIssue 2: Survival Plot Error (object of type 'symbol' is not subsettable). The ggsurvplot function failed in the reactive Shiny environment.\n\nFix: Replaced survfit() with surv_fit() and explicitly wrapped the plot in a print() command.\n\nIssue 3: Sankey Diagram Visual Imbalance. The left side of the Sankey diagram was visually squashed because incoming funds were much smaller than outgoing volume.\n\nFix: Decoupled the visual drawing weights from the actual dollar amounts. Forced balanced proportions (100% vs 100%) while formatting the text labels to still display the actual, accurate currency amounts ($X,XXX,XXX).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTask\nAssignee\nDeadline\nStatus\n\n\n\n\nFinal code cleanup and commenting\nLIN XIANWEI, SU HUI\n29/03/2026\nCompleted\n\n\nDeploy stable version to Shinyapps.io\nLIN XIANWEI, SU HUI\n20/03/2026\nCompleted\n\n\nWrite project proposal and Quarto site\nLIN XIANWEI, SU HUI\n15/03/2026\nCompleted\n\n\n\n\nMinutes prepared by: TEAM 15 - LIN XIANWEI, SU HUI"
  },
  {
    "objectID": "proposal.html",
    "href": "proposal.html",
    "title": "Project Proposal: FinRetain Customer Visual Analytics",
    "section": "",
    "text": "In the highly competitive financial technology (FinTech) and banking sector, customer retention is a critical driver of profitability. Acquiring a new customer is significantly more expensive than retaining an existing one. However, identifying exactly when and why a customer might abandon a financial platform (churn) remains a complex challenge due to the high-dimensional nature of transactional data.\nFinRetain is designed to address this critical blind spot. It is an interactive visual analytics platform built to help financial analysts and product stakeholders dynamically explore customer demographics, track behavioral friction points, and proactively predict churn using advanced statistical modeling."
  },
  {
    "objectID": "proposal.html#introduction-and-motivation",
    "href": "proposal.html#introduction-and-motivation",
    "title": "Project Proposal: FinRetain Customer Visual Analytics",
    "section": "",
    "text": "In the highly competitive financial technology (FinTech) and banking sector, customer retention is a critical driver of profitability. Acquiring a new customer is significantly more expensive than retaining an existing one. However, identifying exactly when and why a customer might abandon a financial platform (churn) remains a complex challenge due to the high-dimensional nature of transactional data.\nFinRetain is designed to address this critical blind spot. It is an interactive visual analytics platform built to help financial analysts and product stakeholders dynamically explore customer demographics, track behavioral friction points, and proactively predict churn using advanced statistical modeling."
  },
  {
    "objectID": "proposal.html#project-objectives",
    "href": "proposal.html#project-objectives",
    "title": "Project Proposal: FinRetain Customer Visual Analytics",
    "section": "2 2. Project Objectives",
    "text": "2 2. Project Objectives\nOur primary goal is to build an interactive R Shiny application that transforms raw transactional and demographic data into actionable business intelligence. Specifically, this project aims to transition from descriptive reporting to proactive forecasting through:\n\nBehavioral Exploration: Provide interactive visual tools to understand user demographic profiles and identify friction points in the user experience.\nAlgorithmic Profiling (K-Means): Autonomously group users into distinct behavioral personas based on transaction frequency and monetary volume.\nAdvanced Segmentation (RFM): Classify the customer base into actionable business segments based on Recency, Frequency, and Monetary metrics.\nTime-to-Event Analysis: Utilize Survival Analysis to determine the exact tenure timeline where specific customer groups are at the highest risk of dropping off.\nMacro Cash Flow Mapping: Visualize the holistic ecosystem of liquidity entering and exiting the platform.\nPredictive Risk Assessment: Implement a dynamic simulator to forecast churn probability and estimate Customer Lifetime Value (CLV) based on adjustable inputs."
  },
  {
    "objectID": "proposal.html#data-description",
    "href": "proposal.html#data-description",
    "title": "Project Proposal: FinRetain Customer Visual Analytics",
    "section": "3 3. Data Description",
    "text": "3 3. Data Description\nThe dataset powering FinRetain contains historical records of customer accounts and transactional logs, encompassing:\n\nDemographics: Age, Gender, Education Level, Income Bracket.\nEngagement Metrics: App Logins, Support Tickets Logged, Satisfaction Score (1-5).\nTransactional Data: Transaction counts, average transaction values, and aggregated volumes across various types (Deposits, Payments, Transfers, Withdrawals).\nTarget Variables: Customer Tenure (months), Churn Probability, and Customer Lifetime Value (CLV)."
  },
  {
    "objectID": "proposal.html#proposed-methodology-analytical-approach",
    "href": "proposal.html#proposed-methodology-analytical-approach",
    "title": "Project Proposal: FinRetain Customer Visual Analytics",
    "section": "4 4. Proposed Methodology & Analytical Approach",
    "text": "4 4. Proposed Methodology & Analytical Approach\nTo achieve our objectives, the FinRetain platform will be engineered using R Shiny and segmented into six distinct analytical modules:\n\n4.1 4.1 Exploratory Data Analysis (EDA)\nUsing ggplot2 and plotly, we will analyze the demographic distribution of our user base. This module will allow stakeholders to filter by Gender and Customer Segments to uncover baseline trends in Income and Education.\n\n\n4.2 4.2 Confirmatory Data Analysis (CDA)\nWe will implement interactive scatter plots with statistical trendlines to investigate behavioral hypotheses, specifically assessing the negative correlation between Support Tickets and overall App Satisfaction for high-volume users.\n\n\n4.3 4.3 Behavioral Clustering\nUsing the K-Means clustering algorithm, we will identify hidden user personas. By plotting transaction frequency against total transaction volume, the platform will autonomously categorize users into distinct operational groups (e.g., High-Volume vs.¬†High-Frequency users).\n\n\n4.4 4.4 RFM Value Segmentation\nCustomers will be scored and categorized into segments (e.g., ‚ÄúChampions‚Äù, ‚ÄúLoyal Customers‚Äù, ‚ÄúAt Risk‚Äù) based on Recency, Frequency, and Monetary metrics. A dynamic plotly Treemap will visualize the size and hierarchy of each segment to prioritize marketing efforts.\n\n\n4.5 4.5 Time-to-Event Survival Analysis\nUsing the survival package, we will plot Kaplan-Meier Churn Probability Curves. This module will act as a ‚Äúticking clock,‚Äù allowing stakeholders to identify the critical 90-day drop-off window and compare survival rates across different user demographics.\n\n\n4.6 4.6 Macro Cash Flow (Sankey Diagram)\nUsing the networkD3 library, we will map the directional flow of capital. The Sankey diagram will visualize how external funds (Direct Deposits) pool into the app wallet and disperse into out-flows, quantifying the exact dollar amounts retained versus lost.\n\n\n4.7 4.7 Churn & CLV Risk Simulator\nWe will train a Multiple Linear Regression (lm) model to predict the probability of churn. A dynamic UI simulator will allow users to adjust hypothetical customer parameters (e.g., simulating a reduction in support tickets) to see real-time shifts in risk metrics, visually forecasting a ‚Äúsaved‚Äù customer."
  },
  {
    "objectID": "proposal.html#proposed-deliverables",
    "href": "proposal.html#proposed-deliverables",
    "title": "Project Proposal: FinRetain Customer Visual Analytics",
    "section": "5 5. Proposed Deliverables",
    "text": "5 5. Proposed Deliverables\n\nR Shiny Application: A fully deployed, interactive web application hosted on shinyapps.io featuring a low-cognitive-load UI designed with bslib.\nProject Report/Website: A comprehensive Quarto document detailing the technical architecture, methodology, business findings, and user guide.\nSource Code Repository: A clean, well-commented GitHub repository containing the ETL data pipeline (app_data.rds) and application logic (app.R)."
  },
  {
    "objectID": "proposal.html#project-timeline",
    "href": "proposal.html#project-timeline",
    "title": "Project Proposal: FinRetain Customer Visual Analytics",
    "section": "6 6. Project Timeline",
    "text": "6 6. Project Timeline\n\n\n\n\n\n\n\n\nPhase\nTask\nExpected Completion\n\n\n\n\n1\nData Extraction, ETL Pipeline & Feature Engineering\n21/02/2026\n\n\n2\nCore UI/UX Dashboard Layout Design\n25/02/2026\n\n\n3\nImplementation of EDA, CDA & Clustering Modules\n01/03/2026\n\n\n4\nAdvanced Analytics Integration (Survival, RFM, Sankey)\n15/03/2026\n\n\n5\nPredictive Simulator Implementation & Final Testing\n29/03/2026\n\n\n6\nFinal Quarto Report Submission & ShinyApps Deployment\n04/04/2026"
  },
  {
    "objectID": "ui_design.html",
    "href": "ui_design.html",
    "title": "Proposed UI Design & Architecture",
    "section": "",
    "text": "The FinRetain dashboard is designed to transform complex financial data into intuitive, actionable insights. To achieve this, our User Interface (UI) architecture strictly follows the principles of reducing cognitive load and maximizing interactivity. The application is divided into a consistent global shell and modular workspaces.\n\n\n\nTo maintain context across all analytical tasks, the dashboard utilizes a Sidebar Layout.\n\nSidebar Panel (Left, 25% width): This is the persistent control center. It contains the primary navigation menu to switch between modules, as well as global filters (Date Range, Customer Segment, Income Level) that apply consistently across the entire app.\nMain Canvas (Right, 75% width): The dynamic viewing area where visualizations are rendered. This maximizes horizontal screen real estate, which is crucial for wide charts like timelines and Sankey diagrams.\n\n\n\n\n\n\n\nGoal: Allow users to quickly slice demographic and behavioral data.\nUI Components:\n\nTop Row (ValueBoxes): Key Performance Indicators (KPIs) such as Total Users, Average Satisfaction, and Churn Rate displayed as highly visible cards.\nControl Bar: Dropdown menus specifically for the plots (e.g., ‚ÄúSelect X-Axis‚Äù, ‚ÄúSelect Y-Axis‚Äù) placed immediately above the charts.\nPlot Area: A 2x2 grid layout for static or interactive plotly charts, ensuring multiple distributions can be compared simultaneously without scrolling.\n\n\n\n\nGoal: Maximize space for dense, high-dimensional visualizations. UI Components: * Full-Width Canvas: For the Macro Cash Flow Sankey diagram, the UI dynamically collapses secondary menus to grant the visualization 100% of the main panel width. * Interactive Tooltips: Instead of cluttering the screen with legends, the UI relies on hover-triggered tooltips. Hovering over a node in the Treemap or Sankey dims the rest of the screen and brings the exact data metrics to the foreground.\n\n\n\nGoal: Create a ‚ÄúWhat-If‚Äù sandbox for stakeholders to interact with our underlying regression and machine learning models.\nUI Components:\n\nInput Column (Left): A vertical stack of interactive sliderInput and selectInput widgets. These represent the specific attributes of a hypothetical customer (e.g., ‚ÄúSupport Tickets Count‚Äù, ‚ÄúTransaction Frequency‚Äù).\nOutput Gauges (Right Top): Real-time, color-coded gauge charts. As the user moves a slider, a gauge instantly updates to show the new ‚ÄúChurn Probability‚Äù (Green = Safe, Red = High Risk).\nText/Table Output (Right Bottom): A dynamically updating text box that outputs the specific projected Customer Lifetime Value (CLV) in dollars based on the simulation.\n\n\n\n\n\n\n\nConsistency: Color palettes are standardized across all modules (e.g., Red always indicates high churn risk or negative sentiment; Blue indicates stability).\nError Prevention: Dropdowns and sliders are constrained to logical limits (e.g., you cannot select a negative number of transactions) to prevent the R server from crashing.\nFeedback: Loading spinners (shinycssloaders) are integrated into the UI so the user knows when the K-Means or Predictive models are calculating in the background."
  },
  {
    "objectID": "ui_design.html#global-architecture-the-app-shell",
    "href": "ui_design.html#global-architecture-the-app-shell",
    "title": "Proposed UI Design & Architecture",
    "section": "",
    "text": "To maintain context across all analytical tasks, the dashboard utilizes a Sidebar Layout.\n\nSidebar Panel (Left, 25% width): This is the persistent control center. It contains the primary navigation menu to switch between modules, as well as global filters (Date Range, Customer Segment, Income Level) that apply consistently across the entire app.\nMain Canvas (Right, 75% width): The dynamic viewing area where visualizations are rendered. This maximizes horizontal screen real estate, which is crucial for wide charts like timelines and Sankey diagrams."
  },
  {
    "objectID": "ui_design.html#module-uis-component-breakdown",
    "href": "ui_design.html#module-uis-component-breakdown",
    "title": "Proposed UI Design & Architecture",
    "section": "",
    "text": "Goal: Allow users to quickly slice demographic and behavioral data.\nUI Components:\n\nTop Row (ValueBoxes): Key Performance Indicators (KPIs) such as Total Users, Average Satisfaction, and Churn Rate displayed as highly visible cards.\nControl Bar: Dropdown menus specifically for the plots (e.g., ‚ÄúSelect X-Axis‚Äù, ‚ÄúSelect Y-Axis‚Äù) placed immediately above the charts.\nPlot Area: A 2x2 grid layout for static or interactive plotly charts, ensuring multiple distributions can be compared simultaneously without scrolling.\n\n\n\n\nGoal: Maximize space for dense, high-dimensional visualizations. UI Components: * Full-Width Canvas: For the Macro Cash Flow Sankey diagram, the UI dynamically collapses secondary menus to grant the visualization 100% of the main panel width. * Interactive Tooltips: Instead of cluttering the screen with legends, the UI relies on hover-triggered tooltips. Hovering over a node in the Treemap or Sankey dims the rest of the screen and brings the exact data metrics to the foreground.\n\n\n\nGoal: Create a ‚ÄúWhat-If‚Äù sandbox for stakeholders to interact with our underlying regression and machine learning models.\nUI Components:\n\nInput Column (Left): A vertical stack of interactive sliderInput and selectInput widgets. These represent the specific attributes of a hypothetical customer (e.g., ‚ÄúSupport Tickets Count‚Äù, ‚ÄúTransaction Frequency‚Äù).\nOutput Gauges (Right Top): Real-time, color-coded gauge charts. As the user moves a slider, a gauge instantly updates to show the new ‚ÄúChurn Probability‚Äù (Green = Safe, Red = High Risk).\nText/Table Output (Right Bottom): A dynamically updating text box that outputs the specific projected Customer Lifetime Value (CLV) in dollars based on the simulation."
  },
  {
    "objectID": "ui_design.html#design-heuristics-applied",
    "href": "ui_design.html#design-heuristics-applied",
    "title": "Proposed UI Design & Architecture",
    "section": "",
    "text": "Consistency: Color palettes are standardized across all modules (e.g., Red always indicates high churn risk or negative sentiment; Blue indicates stability).\nError Prevention: Dropdowns and sliders are constrained to logical limits (e.g., you cannot select a negative number of transactions) to prevent the R server from crashing.\nFeedback: Loading spinners (shinycssloaders) are integrated into the UI so the user knows when the K-Means or Predictive models are calculating in the background."
  }
]