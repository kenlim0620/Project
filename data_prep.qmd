---
title: "Data Preparation & Feature Engineering"
author: "Team 15"
date: today
format: 
  html:
    theme: flatly
    toc: true
    toc-depth: 2
    code-fold: true
---

# Introduction to the Data Pipeline

To ensure the **FinRetain** Shiny application runs at lightning speed on the web, we intentionally decoupled the heavy data processing from the live application. Our data preparation pipeline cleans raw demographic and transactional data, performs advanced feature engineering, and exports a highly optimized `.rds` file. Below is the step-by-step methodology used to prepare the dataset.

------------------------------------------------------------------------

## Step 1: Data Cleaning & Formatting

We began by loading the raw simulated customer dataset. Categorical variables with intrinsic hierarchy needed to be properly factored so that visual plots (like bar charts) would display in a logical order, rather than alphabetically.

```{r}
library(dplyr)
library(tidyr)

# Load raw dataset
df <- read.csv("customer_data.csv")

# Factor ordinal variables for logical plotting
df$income_bracket <- factor(df$income_bracket, 
                            levels = c("Low", "Medium", "High", "Very High"))
```

## Step 2: Feature Engineering (Advanced Metrics)

To power our Advanced Analytics modules (Survival Analysis and RFM Treemap), we needed to dynamically generate new variables from the raw data.

### 2.1 Binary Churn Flag for Survival Analysis

The Kaplan-Meier survival curves require a definitive "event" (1 for occurred, 0 for censored). Since our raw data provided a continuous `churn_probability`, we established a strict threshold: customers with a probability \> 60% were flagged as a definitive churn event.

```{r}
df <- df %>%
  mutate(
    churn_event = ifelse(churn_probability > 0.6, 1, 0)
  )
```

### 2.2 RFM (Recency, Frequency, Monetary) Scoring

We calculated RFM scores by dividing the customer base into tertiles (using `ntile()`) across three key dimensions:

-   **Recency (R):** Based on `customer_tenure` (proxy for how long they've stayed active).

-   **Frequency (F):** Based on `tx_count` (Total Transactions).

-   **Monetary (M):** Based on `total_tx_volume` (Total volume moved).

```{r}
df <- df %>%
  mutate(
    R_Score = ntile(customer_tenure, 3),
    F_Score = ntile(tx_count, 3),
    M_Score = ntile(total_tx_volume, 3)
  )
```

### 2.3 RFM Segmentation Rules

Using the generated 1-3 scores, we applied conditional logic to segment the user base into actionable business categories:

```{r}
df <- df %>%
  mutate(
    RFM_Segment = case_when(
      R_Score == 3 & F_Score == 3 & M_Score == 3 ~ "Champions",
      R_Score <= 2 & F_Score == 3 & M_Score == 3 ~ "Loyal Customers",
      R_Score == 3 & F_Score <= 2 ~ "Recent Users",
      R_Score == 1 & F_Score == 1 & M_Score == 1 ~ "Lost/Churned Risks",
      TRUE ~ "Average Users"
    )
  )
```

## Step 3: Exporting for Production

Once the dataset was cleaned and enriched with our engineered features, we saved it as an R Data Serialization (`.rds`) file. This format preserves all R-specific attributes (like our factored income levels) and loads significantly faster in the Shiny Server environment than a standard CSV.

```{r}
# Export the final optimized dataset for Shiny
saveRDS(df, "app_data.rds")
```
